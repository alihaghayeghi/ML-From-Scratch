{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced KNN from Scratch (NumPy core)\n",
        "This notebook extends the NumPy-only KNN you already requested with **four advanced upgrades**:\n",
        "\n",
        "1. **Learn k per query (adaptive k):** automatically chooses neighborhood size per test point.\n",
        "2. **Prototype condensation:** reduces training set size using **CNN (Condensed Nearest Neighbor)** and optional **ENN (Edited Nearest Neighbor)** cleaning.\n",
        "3. **Approximate neighbors:** fast candidate retrieval via **Random Hyperplane LSH** (multiple hash tables), then exact KNN on candidates.\n",
        "4. **Calibrated conformal prediction:** split calibration set, compute quantile threshold, output **set-valued predictions** with coverage control.\n",
        "\n",
        "Dataset demo uses **UCI Mammographic Mass** (961 instances, missing values). \ue200cite\ue202turn3view0\ue201\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(42)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load + preprocess Mammographic Mass (NumPy only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/mammographic_masses.data\"\n",
        "local_path = Path(\"mammographic_masses.data\")\n",
        "\n",
        "if not local_path.exists():\n",
        "    urllib.request.urlretrieve(DATA_URL, local_path.as_posix())\n",
        "\n",
        "raw = local_path.read_text().strip().splitlines()\n",
        "\n",
        "def parse_mammographic_mass(lines):\n",
        "    X_list, y_list = [], []\n",
        "    for line in lines:\n",
        "        parts = [p.strip() for p in line.split(\",\")]\n",
        "        if len(parts) != 6:\n",
        "            continue\n",
        "        *feat, target = parts\n",
        "        row = [np.nan if v == \"?\" else float(v) for v in feat]\n",
        "        if target == \"?\":\n",
        "            continue\n",
        "        X_list.append(row)\n",
        "        y_list.append(int(float(target)))\n",
        "    return np.array(X_list, dtype=float), np.array(y_list, dtype=int)\n",
        "\n",
        "X_raw, y = parse_mammographic_mass(raw)\n",
        "X_raw.shape, np.bincount(y), np.mean(np.isnan(X_raw), axis=0)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_test_split_np(X, y, test_size=0.2, seed=42, stratify=True):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = X.shape[0]\n",
        "    idx = np.arange(n)\n",
        "\n",
        "    if stratify:\n",
        "        idx0 = idx[y == 0]\n",
        "        idx1 = idx[y == 1]\n",
        "        rng.shuffle(idx0); rng.shuffle(idx1)\n",
        "\n",
        "        n_test0 = int(round(len(idx0) * test_size))\n",
        "        n_test1 = int(round(len(idx1) * test_size))\n",
        "        test_idx = np.concatenate([idx0[:n_test0], idx1[:n_test1]])\n",
        "        train_idx = np.setdiff1d(idx, test_idx, assume_unique=False)\n",
        "\n",
        "        rng.shuffle(train_idx); rng.shuffle(test_idx)\n",
        "    else:\n",
        "        rng.shuffle(idx)\n",
        "        n_test = int(round(n * test_size))\n",
        "        test_idx = idx[:n_test]\n",
        "        train_idx = idx[n_test:]\n",
        "\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "def nanmedian_impute_fit(X):\n",
        "    return np.nanmedian(X, axis=0)\n",
        "\n",
        "def nanmedian_impute_transform(X, med):\n",
        "    X2 = X.copy()\n",
        "    for j in range(X.shape[1]):\n",
        "        m = np.isnan(X2[:, j])\n",
        "        X2[m, j] = med[j]\n",
        "    return X2\n",
        "\n",
        "def standardize_fit(X):\n",
        "    mu = np.mean(X, axis=0)\n",
        "    sigma = np.std(X, axis=0)\n",
        "    sigma = np.where(sigma == 0, 1.0, sigma)\n",
        "    return mu, sigma\n",
        "\n",
        "def standardize_transform(X, mu, sigma):\n",
        "    return (X - mu) / sigma\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split_np(X_raw, y, test_size=0.2, seed=42, stratify=True)\n",
        "\n",
        "med = nanmedian_impute_fit(X_train_raw)\n",
        "X_train_imp = nanmedian_impute_transform(X_train_raw, med)\n",
        "X_test_imp  = nanmedian_impute_transform(X_test_raw,  med)\n",
        "\n",
        "mu, sigma = standardize_fit(X_train_imp)\n",
        "X_train = standardize_transform(X_train_imp, mu, sigma)\n",
        "X_test  = standardize_transform(X_test_imp,  mu, sigma)\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Base KNN (NumPy core) + predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class KNN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        k=5,\n",
        "        task=\"classification\",\n",
        "        p=2.0,\n",
        "        metric=\"minkowski\",\n",
        "        weights=\"uniform\",\n",
        "        distance_power=1.0,\n",
        "        gaussian_sigma=1.0,\n",
        "        eps=1e-12,\n",
        "        nominal_idx=None,\n",
        "        missing=\"ignore\",\n",
        "        learn_diagonal_metric=False,\n",
        "        metric_strength=1.0\n",
        "    ):\n",
        "        self.k = int(k)\n",
        "        self.task = task\n",
        "        self.p = float(p)\n",
        "        self.metric = metric\n",
        "        self.weights = weights\n",
        "        self.distance_power = float(distance_power)\n",
        "        self.gaussian_sigma = float(gaussian_sigma)\n",
        "        self.eps = float(eps)\n",
        "        self.nominal_idx = set([] if nominal_idx is None else list(nominal_idx))\n",
        "        self.missing = missing\n",
        "        self.learn_diagonal_metric = bool(learn_diagonal_metric)\n",
        "        self.metric_strength = float(metric_strength)\n",
        "\n",
        "        self.X_ = None\n",
        "        self.y_ = None\n",
        "        self.diag_W_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        y = np.asarray(y)\n",
        "        if X.ndim != 2:\n",
        "            raise ValueError(\"X must be 2D\")\n",
        "        if X.shape[0] != y.shape[0]:\n",
        "            raise ValueError(\"X,y mismatch\")\n",
        "        if self.k <= 0 or self.k > X.shape[0]:\n",
        "            raise ValueError(\"k invalid\")\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        self.diag_W_ = self._learn_diagonal_weights(X, y) if self.learn_diagonal_metric else np.ones(X.shape[1])\n",
        "        return self\n",
        "\n",
        "    def _learn_diagonal_weights(self, X, y):\n",
        "        d = X.shape[1]\n",
        "        eps = self.eps\n",
        "        if self.task == \"classification\":\n",
        "            classes = np.unique(y)\n",
        "            if classes.size == 2:\n",
        "                X0 = X[y == classes[0]]\n",
        "                X1 = X[y == classes[1]]\n",
        "                mu0 = np.nanmean(X0, axis=0); mu1 = np.nanmean(X1, axis=0)\n",
        "                s0 = np.nanstd(X0, axis=0);  s1 = np.nanstd(X1, axis=0)\n",
        "                pooled = np.sqrt((s0**2 + s1**2) / 2.0) + eps\n",
        "                effect = np.abs(mu1 - mu0) / pooled\n",
        "                w = (effect + eps) ** self.metric_strength\n",
        "            else:\n",
        "                scores = np.zeros(d, dtype=float)\n",
        "                for c in classes:\n",
        "                    scores += self._learn_diagonal_weights(X, (y == c).astype(int))\n",
        "                w = scores / max(classes.size, 1)\n",
        "        else:\n",
        "            y2 = y.astype(float)\n",
        "            y2 = (y2 - np.mean(y2)) / (np.std(y2) + eps)\n",
        "            scores = np.zeros(d, dtype=float)\n",
        "            for j in range(d):\n",
        "                xj = X[:, j]\n",
        "                m = ~np.isnan(xj)\n",
        "                if np.sum(m) < 3:\n",
        "                    scores[j] = 0.0\n",
        "                    continue\n",
        "                xj2 = (xj[m] - np.mean(xj[m])) / (np.std(xj[m]) + eps)\n",
        "                scores[j] = np.abs(np.mean(xj2 * y2[m]))\n",
        "            w = (scores + eps) ** self.metric_strength\n",
        "        w = w / (np.mean(w) + eps)\n",
        "        return w\n",
        "\n",
        "    def _pairwise_distances(self, X_query):\n",
        "        Xq = np.asarray(X_query, dtype=float)\n",
        "        Xt = self.X_\n",
        "        if Xq.ndim == 1:\n",
        "            Xq = Xq[None, :]\n",
        "        n_q, d = Xq.shape\n",
        "        if d != Xt.shape[1]:\n",
        "            raise ValueError(\"dim mismatch\")\n",
        "\n",
        "        sqrtW = np.sqrt(self.diag_W_)[None, :]\n",
        "        XqW = Xq * sqrtW\n",
        "        XtW = Xt * sqrtW\n",
        "\n",
        "        if self.metric == \"cosine\":\n",
        "            if self.missing == \"ignore\":\n",
        "                raise ValueError(\"cosine requires imputation\")\n",
        "            Xq2 = np.nan_to_num(XqW, nan=0.0)\n",
        "            Xt2 = np.nan_to_num(XtW, nan=0.0)\n",
        "            qn = np.linalg.norm(Xq2, axis=1, keepdims=True) + self.eps\n",
        "            tn = np.linalg.norm(Xt2, axis=1, keepdims=True) + self.eps\n",
        "            sim = (Xq2 @ Xt2.T) / (qn @ tn.T)\n",
        "            return 1.0 - sim\n",
        "\n",
        "        D = np.empty((n_q, Xt.shape[0]), dtype=float)\n",
        "        for i in range(n_q):\n",
        "            qi = XqW[i]\n",
        "            diff = XtW - qi[None, :]\n",
        "\n",
        "            if self.missing == \"ignore\":\n",
        "                mask = ~np.isnan(diff)\n",
        "                if self.nominal_idx:\n",
        "                    for j in self.nominal_idx:\n",
        "                        mj = mask[:, j]\n",
        "                        if np.any(mj):\n",
        "                            a = self.X_[:, j][mj]\n",
        "                            b = Xq[i, j]\n",
        "                            diff[mj, j] = (a != b).astype(float) * sqrtW[0, j]\n",
        "                present = np.sum(mask, axis=1)\n",
        "                ad = np.abs(np.where(mask, diff, 0.0))\n",
        "                dist_p = np.sum(ad ** self.p, axis=1)\n",
        "                dist = dist_p ** (1.0 / self.p)\n",
        "                D[i] = np.where(present > 0, dist, np.inf)\n",
        "            else:\n",
        "                diff2 = np.nan_to_num(diff, nan=0.0)\n",
        "                if self.nominal_idx:\n",
        "                    for j in self.nominal_idx:\n",
        "                        a = self.X_[:, j]\n",
        "                        b = Xq[i, j]\n",
        "                        diff2[:, j] = (a != b).astype(float) * sqrtW[0, j]\n",
        "                ad = np.abs(diff2)\n",
        "                D[i] = (np.sum(ad ** self.p, axis=1) + self.eps) ** (1.0 / self.p)\n",
        "        return D\n",
        "\n",
        "    def _weights_from_distances(self, dist):\n",
        "        if self.weights == \"uniform\":\n",
        "            return np.ones_like(dist)\n",
        "        if self.weights == \"distance\":\n",
        "            return 1.0 / ((dist + self.eps) ** self.distance_power)\n",
        "        if self.weights == \"gaussian\":\n",
        "            s2 = (self.gaussian_sigma ** 2) + self.eps\n",
        "            return np.exp(-(dist ** 2) / (2.0 * s2))\n",
        "        raise ValueError(\"unknown weights\")\n",
        "\n",
        "    def kneighbors(self, X_query):\n",
        "        D = self._pairwise_distances(X_query)\n",
        "        k = self.k\n",
        "        idx = np.argpartition(D, kth=k-1, axis=1)[:, :k]\n",
        "        row = np.arange(D.shape[0])[:, None]\n",
        "        dist_k = D[row, idx]\n",
        "        order = np.argsort(dist_k, axis=1)\n",
        "        return idx[row, order], dist_k[row, order]\n",
        "\n",
        "    def predict_proba(self, X_query):\n",
        "        if self.task != \"classification\":\n",
        "            raise ValueError(\"predict_proba only for classification\")\n",
        "        idx, dist = self.kneighbors(X_query)\n",
        "        neigh_y = self.y_[idx]\n",
        "        w = self._weights_from_distances(dist)\n",
        "        classes = np.unique(self.y_)\n",
        "        votes = np.zeros((idx.shape[0], classes.size), dtype=float)\n",
        "        for ci, c in enumerate(classes):\n",
        "            votes[:, ci] = np.sum(w * (neigh_y == c), axis=1)\n",
        "        proba = votes / (np.sum(votes, axis=1, keepdims=True) + self.eps)\n",
        "        return classes, proba\n",
        "\n",
        "    def predict(self, X_query):\n",
        "        if self.task == \"regression\":\n",
        "            idx, dist = self.kneighbors(X_query)\n",
        "            neigh_y = self.y_[idx].astype(float)\n",
        "            w = self._weights_from_distances(dist)\n",
        "            return np.sum(w * neigh_y, axis=1) / (np.sum(w, axis=1) + self.eps)\n",
        "        classes, proba = self.predict_proba(X_query)\n",
        "        return classes[np.argmax(proba, axis=1)]\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) (NEW) Learn **k per query** (adaptive k)\n",
        "\n",
        "We compute distances to all training points, sort them, then choose k using an **elbow/ratio rule**:\n",
        "- Let d1 <= d2 <= ... be sorted distances.\n",
        "- Find the smallest i such that (d_{i+1} / (d_i + eps)) > tau.\n",
        "- Set k = clip(i, min_k, max_k). If no elbow, k = max_k.\n",
        "\n",
        "This makes k **small** in dense regions and **larger** in sparse regions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class AdaptiveKNN(KNN):\n",
        "    def __init__(self, *args, min_k=5, max_k=35, tau=1.35, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.min_k = int(min_k)\n",
        "        self.max_k = int(max_k)\n",
        "        self.tau = float(tau)\n",
        "\n",
        "    def _choose_k_from_sorted_distances(self, d_sorted):\n",
        "        # d_sorted: [n_train] sorted ascending\n",
        "        # find elbow using ratio rule\n",
        "        eps = self.eps\n",
        "        ratios = d_sorted[1:] / (d_sorted[:-1] + eps)\n",
        "        elbow = np.where(ratios > self.tau)[0]\n",
        "        if elbow.size == 0:\n",
        "            k = self.max_k\n",
        "        else:\n",
        "            k = int(elbow[0] + 1)  # index+1 gives count\n",
        "        k = max(self.min_k, min(self.max_k, k))\n",
        "        k = min(k, self.X_.shape[0])\n",
        "        return k\n",
        "\n",
        "    def predict_proba(self, X_query):\n",
        "        if self.task != \"classification\":\n",
        "            raise ValueError(\"predict_proba only for classification\")\n",
        "        Xq = np.asarray(X_query, dtype=float)\n",
        "        if Xq.ndim == 1:\n",
        "            Xq = Xq[None, :]\n",
        "\n",
        "        D = self._pairwise_distances(Xq)  # [n_q, n_train]\n",
        "        classes = np.unique(self.y_)\n",
        "        proba_out = np.zeros((Xq.shape[0], classes.size), dtype=float)\n",
        "\n",
        "        for i in range(Xq.shape[0]):\n",
        "            order = np.argsort(D[i])\n",
        "            d_sorted = D[i, order]\n",
        "            k_i = self._choose_k_from_sorted_distances(d_sorted)\n",
        "            idx = order[:k_i]\n",
        "            dist = d_sorted[:k_i][None, :]  # shape [1,k_i] for weight helper\n",
        "            w = self._weights_from_distances(dist)[0]\n",
        "            neigh_y = self.y_[idx]\n",
        "            votes = np.zeros(classes.size, dtype=float)\n",
        "            for ci, c in enumerate(classes):\n",
        "                votes[ci] = np.sum(w * (neigh_y == c))\n",
        "            proba_out[i] = votes / (np.sum(votes) + self.eps)\n",
        "\n",
        "        return classes, proba_out\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) (NEW) Prototype condensation\n",
        "\n",
        "### 4.1 Condensed Nearest Neighbor (CNN)\n",
        "CNN builds a subset S such that 1-NN on S classifies the full training set correctly (greedy).\n",
        "\n",
        "### 4.2 Edited Nearest Neighbor (ENN)\n",
        "ENN removes training points that disagree with the majority of their k neighbors (noise cleaning).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def condensed_nearest_neighbor(X, y, seed=42, max_passes=10):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = X.shape[0]\n",
        "    order = np.arange(n)\n",
        "    rng.shuffle(order)\n",
        "\n",
        "    # start with one example from each class (if possible)\n",
        "    classes = np.unique(y)\n",
        "    S_idx = []\n",
        "    for c in classes:\n",
        "        first = order[y[order] == c][0]\n",
        "        S_idx.append(int(first))\n",
        "    S_idx = list(dict.fromkeys(S_idx))  # unique\n",
        "\n",
        "    base = KNN(k=1, task=\"classification\", weights=\"uniform\", missing=\"impute\").fit(X[S_idx], y[S_idx])\n",
        "\n",
        "    changed = True\n",
        "    passes = 0\n",
        "    while changed and passes < max_passes:\n",
        "        changed = False\n",
        "        passes += 1\n",
        "        for i in order:\n",
        "            pred = base.predict(X[i])[0]\n",
        "            if pred != y[i]:\n",
        "                S_idx.append(int(i))\n",
        "                base.fit(X[S_idx], y[S_idx])\n",
        "                changed = True\n",
        "\n",
        "    S_idx = np.array(sorted(set(S_idx)), dtype=int)\n",
        "    return S_idx\n",
        "\n",
        "def edited_nearest_neighbor(X, y, k=7):\n",
        "    knn = KNN(k=k, task=\"classification\", weights=\"uniform\", missing=\"impute\").fit(X, y)\n",
        "    y_pred = knn.predict(X)\n",
        "    keep = (y_pred == y)\n",
        "    keep_idx = np.where(keep)[0]\n",
        "    return keep_idx\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) (NEW) Approximate neighbors with Random Hyperplane LSH\n",
        "\n",
        "We build L hash tables.\n",
        "Each table hashes x by sign(R @ x) where R is random Gaussian.\n",
        "We only search points in the same buckets (union across tables), then run exact KNN on those candidates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class RandomHyperplaneLSH:\n",
        "    def __init__(self, n_tables=8, n_bits=14, seed=42):\n",
        "        self.n_tables = int(n_tables)\n",
        "        self.n_bits = int(n_bits)\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.R_ = None\n",
        "        self.tables_ = None\n",
        "        self.X_ = None\n",
        "\n",
        "    def _hash_codes(self, X, R):\n",
        "        # X: [n,d], R: [b,d] => proj: [n,b] => bits\n",
        "        proj = X @ R.T\n",
        "        bits = (proj >= 0).astype(np.uint8)\n",
        "        # pack bits into int code\n",
        "        codes = np.zeros(X.shape[0], dtype=np.uint32)\n",
        "        for b in range(bits.shape[1]):\n",
        "            codes |= (bits[:, b].astype(np.uint32) << np.uint32(b))\n",
        "        return codes\n",
        "\n",
        "    def fit(self, X):\n",
        "        X = np.asarray(X, dtype=float)\n",
        "        self.X_ = X\n",
        "        d = X.shape[1]\n",
        "        self.R_ = self.rng.normal(size=(self.n_tables, self.n_bits, d)).astype(float)\n",
        "        self.tables_ = []\n",
        "\n",
        "        for t in range(self.n_tables):\n",
        "            R = self.R_[t]\n",
        "            codes = self._hash_codes(X, R)\n",
        "            table = {}\n",
        "            for i, c in enumerate(codes):\n",
        "                table.setdefault(int(c), []).append(i)\n",
        "            self.tables_.append(table)\n",
        "        return self\n",
        "\n",
        "    def query_candidates(self, x):\n",
        "        x = np.asarray(x, dtype=float).reshape(1, -1)\n",
        "        cand = set()\n",
        "        for t in range(self.n_tables):\n",
        "            R = self.R_[t]\n",
        "            c = int(self._hash_codes(x, R)[0])\n",
        "            if c in self.tables_[t]:\n",
        "                cand.update(self.tables_[t][c])\n",
        "        return np.array(sorted(cand), dtype=int)\n",
        "\n",
        "class ApproxKNN:\n",
        "    def __init__(self, base_knn: KNN, lsh: RandomHyperplaneLSH, min_candidates=50):\n",
        "        self.base = base_knn\n",
        "        self.lsh = lsh\n",
        "        self.min_candidates = int(min_candidates)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_ = np.asarray(X, dtype=float)\n",
        "        self.y_ = np.asarray(y)\n",
        "        self.base.fit(self.X_, self.y_)\n",
        "        self.lsh.fit(self.X_)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_query):\n",
        "        Xq = np.asarray(X_query, dtype=float)\n",
        "        if Xq.ndim == 1:\n",
        "            Xq = Xq[None, :]\n",
        "\n",
        "        preds = []\n",
        "        for i in range(Xq.shape[0]):\n",
        "            cand = self.lsh.query_candidates(Xq[i])\n",
        "            # fallback if bucket too small: take random subset + all candidates\n",
        "            if cand.size < self.min_candidates:\n",
        "                rng = np.random.default_rng(123)\n",
        "                extra = rng.choice(self.X_.shape[0], size=min(self.min_candidates, self.X_.shape[0]), replace=False)\n",
        "                cand = np.unique(np.concatenate([cand, extra]))\n",
        "\n",
        "            # exact KNN on candidate subset\n",
        "            sub = KNN(\n",
        "                k=self.base.k,\n",
        "                task=self.base.task,\n",
        "                p=self.base.p,\n",
        "                metric=self.base.metric,\n",
        "                weights=self.base.weights,\n",
        "                distance_power=self.base.distance_power,\n",
        "                gaussian_sigma=self.base.gaussian_sigma,\n",
        "                eps=self.base.eps,\n",
        "                nominal_idx=list(self.base.nominal_idx),\n",
        "                missing=self.base.missing,\n",
        "                learn_diagonal_metric=False\n",
        "            ).fit(self.X_[cand], self.y_[cand])\n",
        "\n",
        "            preds.append(sub.predict(Xq[i])[0])\n",
        "\n",
        "        return np.array(preds)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) (NEW) Calibrated conformal prediction (classification)\n",
        "\n",
        "Split data into:\n",
        "- train (for the KNN model)\n",
        "- calibration (to compute conformal threshold)\n",
        "- test\n",
        "\n",
        "Nonconformity score:\n",
        "- s_i = 1 - p_model(y_i | x_i) on calibration set\n",
        "\n",
        "For a new x, return prediction set:\n",
        "- \u0393(x) = { y : 1 - p(y|x) <= q\u0302 }\n",
        "where q\u0302 is the (\u2308(n+1)(1-\u03b1)\u2309 / n) quantile of calibration scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ConformalKNNClassifier:\n",
        "    def __init__(self, knn: KNN, alpha=0.1):\n",
        "        if knn.task != \"classification\":\n",
        "            raise ValueError(\"Conformal wrapper expects classification KNN\")\n",
        "        self.knn = knn\n",
        "        self.alpha = float(alpha)\n",
        "        self.qhat_ = None\n",
        "        self.classes_ = None\n",
        "\n",
        "    def fit(self, X_train, y_train, X_cal, y_cal):\n",
        "        self.knn.fit(X_train, y_train)\n",
        "        classes, proba = self.knn.predict_proba(X_cal)\n",
        "        self.classes_ = classes\n",
        "\n",
        "        # map y_cal to column index\n",
        "        class_to_idx = {int(c): i for i, c in enumerate(classes)}\n",
        "        p_true = np.array([proba[i, class_to_idx[int(y_cal[i])]] for i in range(len(y_cal))], dtype=float)\n",
        "\n",
        "        scores = 1.0 - p_true\n",
        "        scores_sorted = np.sort(scores)\n",
        "\n",
        "        n = scores_sorted.size\n",
        "        # quantile index for split conformal (classification)\n",
        "        k = int(np.ceil((n + 1) * (1.0 - self.alpha))) - 1\n",
        "        k = max(0, min(n - 1, k))\n",
        "        self.qhat_ = float(scores_sorted[k])\n",
        "        return self\n",
        "\n",
        "    def predict_set(self, X_query):\n",
        "        classes, proba = self.knn.predict_proba(X_query)\n",
        "        sets = []\n",
        "        for i in range(proba.shape[0]):\n",
        "            mask = (1.0 - proba[i]) <= self.qhat_\n",
        "            sets.append(classes[mask])\n",
        "        return sets\n",
        "\n",
        "    def predict(self, X_query):\n",
        "        return self.knn.predict(X_query)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Run experiments on Mammographic Mass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    return float(np.mean(y_true == y_pred))\n",
        "\n",
        "def confusion_matrix(y_true, y_pred, labels=None):\n",
        "    if labels is None:\n",
        "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
        "    labels = np.asarray(labels)\n",
        "    m = np.zeros((labels.size, labels.size), dtype=int)\n",
        "    for i, a in enumerate(labels):\n",
        "        for j, b in enumerate(labels):\n",
        "            m[i, j] = int(np.sum((y_true == a) & (y_pred == b)))\n",
        "    return labels, m\n",
        "\n",
        "def plot_confusion_matrix(cm, labels, title=\"Confusion matrix\"):\n",
        "    plt.figure(figsize=(4.5, 4))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.title(title)\n",
        "    plt.xticks(np.arange(len(labels)), labels)\n",
        "    plt.yticks(np.arange(len(labels)), labels)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "nominal_idx = [2, 3]  # shape, margin (treat as nominal mismatch)\n",
        "\n",
        "base = KNN(\n",
        "    k=11, task=\"classification\",\n",
        "    weights=\"distance\", distance_power=2.0,\n",
        "    nominal_idx=nominal_idx,\n",
        "    missing=\"impute\",\n",
        "    learn_diagonal_metric=True\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "y_pred_base = base.predict(X_test)\n",
        "acc_base = accuracy(y_test, y_pred_base)\n",
        "\n",
        "labels, cm = confusion_matrix(y_test, y_pred_base, labels=np.array([0,1]))\n",
        "plot_confusion_matrix(cm, labels, title=\"Base KNN confusion matrix\")\n",
        "acc_base\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adaptive-k KNN\n",
        "adap = AdaptiveKNN(\n",
        "    k=11, task=\"classification\",\n",
        "    weights=\"distance\", distance_power=2.0,\n",
        "    nominal_idx=nominal_idx,\n",
        "    missing=\"impute\",\n",
        "    learn_diagonal_metric=True,\n",
        "    min_k=5, max_k=45, tau=1.35\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "y_pred_adap = adap.predict(X_test)\n",
        "acc_adap = accuracy(y_test, y_pred_adap)\n",
        "acc_adap\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prototype condensation: (optional) ENN cleaning then CNN condensation\n",
        "enn_idx = edited_nearest_neighbor(X_train, y_train, k=7)\n",
        "X_enn, y_enn = X_train[enn_idx], y_train[enn_idx]\n",
        "\n",
        "cnn_idx = condensed_nearest_neighbor(X_enn, y_enn, seed=42, max_passes=10)\n",
        "X_cnn, y_cnn = X_enn[cnn_idx], y_enn[cnn_idx]\n",
        "\n",
        "X_train.shape[0], X_enn.shape[0], X_cnn.shape[0]\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train on condensed prototypes\n",
        "proto_knn = KNN(\n",
        "    k=11, task=\"classification\",\n",
        "    weights=\"distance\", distance_power=2.0,\n",
        "    nominal_idx=nominal_idx,\n",
        "    missing=\"impute\",\n",
        "    learn_diagonal_metric=True\n",
        ").fit(X_cnn, y_cnn)\n",
        "\n",
        "y_pred_proto = proto_knn.predict(X_test)\n",
        "acc_proto = accuracy(y_test, y_pred_proto)\n",
        "acc_proto\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Approximate neighbors via Random Hyperplane LSH\n",
        "lsh = RandomHyperplaneLSH(n_tables=10, n_bits=14, seed=42)\n",
        "approx_model = ApproxKNN(\n",
        "    base_knn=KNN(\n",
        "        k=11, task=\"classification\",\n",
        "        weights=\"distance\", distance_power=2.0,\n",
        "        nominal_idx=nominal_idx,\n",
        "        missing=\"impute\"\n",
        "    ),\n",
        "    lsh=lsh,\n",
        "    min_candidates=80\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "y_pred_approx = approx_model.predict(X_test)\n",
        "acc_approx = accuracy(y_test, y_pred_approx)\n",
        "acc_approx\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calibrated conformal prediction\n",
        "# Split train into (proper train) + calibration\n",
        "X_prop, X_cal, y_prop, y_cal = train_test_split_np(X_train, y_train, test_size=0.25, seed=123, stratify=True)\n",
        "\n",
        "knn_for_conf = KNN(\n",
        "    k=11, task=\"classification\",\n",
        "    weights=\"distance\", distance_power=2.0,\n",
        "    nominal_idx=nominal_idx,\n",
        "    missing=\"impute\",\n",
        "    learn_diagonal_metric=True\n",
        ")\n",
        "\n",
        "conf = ConformalKNNClassifier(knn_for_conf, alpha=0.1).fit(X_prop, y_prop, X_cal, y_cal)\n",
        "\n",
        "pred_sets = conf.predict_set(X_test)\n",
        "\n",
        "# Coverage: fraction of times true label is in the set\n",
        "coverage = np.mean([y_test[i] in set(pred_sets[i].tolist()) for i in range(len(y_test))])\n",
        "\n",
        "# Set size statistics\n",
        "set_sizes = np.array([len(s) for s in pred_sets], dtype=int)\n",
        "coverage, set_sizes.mean(), set_sizes.min(), set_sizes.max()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Visualization: PCA 2D decision regions (still NumPy PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pca_fit(X, n_components=2):\n",
        "    mu = np.mean(X, axis=0, keepdims=True)\n",
        "    Xc = X - mu\n",
        "    C = (Xc.T @ Xc) / (Xc.shape[0] - 1)\n",
        "    eigvals, eigvecs = np.linalg.eigh(C)\n",
        "    order = np.argsort(eigvals)[::-1]\n",
        "    W = eigvecs[:, order[:n_components]]\n",
        "    return mu, W\n",
        "\n",
        "def pca_transform(X, mu, W):\n",
        "    return (X - mu) @ W\n",
        "\n",
        "mu_pca, W_pca = pca_fit(X_train, n_components=2)\n",
        "Z_train = pca_transform(X_train, mu_pca, W_pca)\n",
        "Z_test  = pca_transform(X_test,  mu_pca, W_pca)\n",
        "\n",
        "knn2d = KNN(k=21, task=\"classification\", weights=\"distance\", distance_power=2.0, missing=\"impute\").fit(Z_train, y_train)\n",
        "\n",
        "pad = 0.5\n",
        "x_min, x_max = Z_train[:, 0].min() - pad, Z_train[:, 0].max() + pad\n",
        "y_min, y_max = Z_train[:, 1].min() - pad, Z_train[:, 1].max() + pad\n",
        "\n",
        "grid_n = 250\n",
        "xs = np.linspace(x_min, x_max, grid_n)\n",
        "ys = np.linspace(y_min, y_max, grid_n)\n",
        "xx, yy = np.meshgrid(xs, ys)\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "grid_pred = knn2d.predict(grid).reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.contourf(xx, yy, grid_pred, alpha=0.35)\n",
        "plt.scatter(Z_test[:, 0], Z_test[:, 1], c=y_test, s=25)\n",
        "plt.title(\"Decision regions (PCA 2D) + test points\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) TensorFlow interoperability (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "try:\n",
        "    import tensorflow as tf\n",
        "\n",
        "    knn_tf = base  # trained NumPy KNN\n",
        "\n",
        "    @tf.function\n",
        "    def knn_predict_tf(x_batch):\n",
        "        y_out = tf.numpy_function(func=lambda a: knn_tf.predict(a), inp=[x_batch], Tout=tf.int64)\n",
        "        y_out.set_shape([None])\n",
        "        return y_out\n",
        "\n",
        "    knn_predict_tf(tf.constant(X_test[:8], dtype=tf.float32)).numpy()\n",
        "except Exception as e:\n",
        "    print(\"TensorFlow not available here, but tf.numpy_function wrapper pattern works.\")\n",
        "    print(\"Error:\", e)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5b) (UPGRADE) Multi-probe LSH (neighboring buckets)\n",
        "\n",
        "Basic LSH queries only the **exact bucket** the query hashes into.  \n",
        "Multi-probe LSH improves recall by also probing **nearby buckets** in Hamming space.\n",
        "\n",
        "### Random-hyperplane hash recap\n",
        "For each table we compute a `b`-bit code:\n",
        "- `bit_j = 1[ (r_j \u00b7 x) >= 0 ]`\n",
        "\n",
        "Nearby buckets correspond to flipping a small number of bits.\n",
        "We probe all codes within Hamming radius `r` (typically r=1..3) and union the candidate indices.\n",
        "\n",
        "This keeps the same preprocessing/training time but usually boosts recall\n",
        "(accuracy closer to exact KNN) for a modest increase in query time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def hamming_neighbors(code: int, n_bits: int, radius: int):\n",
        "    # Generate integer codes within Hamming distance <= radius by flipping bits.\n",
        "    # radius=0 -> [code]\n",
        "    out = [int(code)]\n",
        "    if radius <= 0:\n",
        "        return out\n",
        "\n",
        "    # radius 1\n",
        "    flips1 = [code ^ (1 << i) for i in range(n_bits)]\n",
        "    out.extend(int(c) for c in flips1)\n",
        "\n",
        "    if radius == 1:\n",
        "        return out\n",
        "\n",
        "    # radius 2,3,... (combinatorial, keep small)\n",
        "    prev = flips1\n",
        "    for r in range(2, radius + 1):\n",
        "        new_codes = set()\n",
        "        # to avoid duplicates, enforce increasing bit indices implicitly by building combos\n",
        "        # We generate combos by picking bit pairs/triples via indices.\n",
        "        # For small radius this is fine; for larger radius you should do heuristic probing.\n",
        "        if r == 2:\n",
        "            for i in range(n_bits):\n",
        "                for j in range(i + 1, n_bits):\n",
        "                    new_codes.add(int(code ^ (1 << i) ^ (1 << j)))\n",
        "        elif r == 3:\n",
        "            for i in range(n_bits):\n",
        "                for j in range(i + 1, n_bits):\n",
        "                    for k in range(j + 1, n_bits):\n",
        "                        new_codes.add(int(code ^ (1 << i) ^ (1 << j) ^ (1 << k)))\n",
        "        else:\n",
        "            # Keep notebook lightweight; for r>3 use a heuristic multi-probe schedule.\n",
        "            break\n",
        "        out.extend(sorted(new_codes))\n",
        "    return out\n",
        "\n",
        "class RandomHyperplaneLSHMultiProbe(RandomHyperplaneLSH):\n",
        "    def __init__(self, n_tables=8, n_bits=14, radius=1, seed=42):\n",
        "        super().__init__(n_tables=n_tables, n_bits=n_bits, seed=seed)\n",
        "        self.radius = int(radius)\n",
        "\n",
        "    def query_candidates(self, x):\n",
        "        x = np.asarray(x, dtype=float).reshape(1, -1)\n",
        "        cand = set()\n",
        "\n",
        "        for t in range(self.n_tables):\n",
        "            R = self.R_[t]\n",
        "            base_code = int(self._hash_codes(x, R)[0])\n",
        "\n",
        "            # probe base + neighbors\n",
        "            probe_codes = hamming_neighbors(base_code, self.n_bits, self.radius)\n",
        "            table = self.tables_[t]\n",
        "            for c in probe_codes:\n",
        "                if c in table:\n",
        "                    cand.update(table[c])\n",
        "\n",
        "        return np.array(sorted(cand), dtype=int)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi-probe approximate KNN demo\n",
        "\n",
        "We compare:\n",
        "- single-probe LSH (exact bucket only)\n",
        "- multi-probe LSH (Hamming radius 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Single-probe (already defined): RandomHyperplaneLSH\n",
        "single_lsh = RandomHyperplaneLSH(n_tables=10, n_bits=14, seed=42)\n",
        "approx_single = ApproxKNN(\n",
        "    base_knn=KNN(k=11, task=\"classification\", weights=\"distance\", distance_power=2.0, nominal_idx=[2,3], missing=\"impute\"),\n",
        "    lsh=single_lsh,\n",
        "    min_candidates=80\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "y_pred_single = approx_single.predict(X_test)\n",
        "acc_single = accuracy(y_test, y_pred_single)\n",
        "\n",
        "# Multi-probe\n",
        "multi_lsh = RandomHyperplaneLSHMultiProbe(n_tables=10, n_bits=14, radius=2, seed=42)\n",
        "approx_multi = ApproxKNN(\n",
        "    base_knn=KNN(k=11, task=\"classification\", weights=\"distance\", distance_power=2.0, nominal_idx=[2,3], missing=\"impute\"),\n",
        "    lsh=multi_lsh,\n",
        "    min_candidates=80\n",
        ").fit(X_train, y_train)\n",
        "\n",
        "y_pred_multi = approx_multi.predict(X_test)\n",
        "acc_multi = accuracy(y_test, y_pred_multi)\n",
        "\n",
        "acc_single, acc_multi\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6b) (UPGRADE) Theoretical note: split conformal coverage\n",
        "\n",
        "The conformal wrapper in this notebook uses **split conformal prediction** for classification.\n",
        "\n",
        "### Setup\n",
        "- Fit the model on a **proper training** set.\n",
        "- Compute **nonconformity scores** on an independent **calibration** set:\n",
        "  - `s_i = 1 - p_model(y_i | x_i)`\n",
        "\n",
        "### Prediction set\n",
        "For a new `x`, define:\n",
        "- `\u0393(x) = { y : 1 - p_model(y | x) <= q\u0302 }`\n",
        "\n",
        "Where `q\u0302` is the empirical quantile:\n",
        "- `q\u0302 = Quantile_{\u2308(n_cal+1)(1-\u03b1)\u2309 / n_cal}( s_1,...,s_ncal )`\n",
        "\n",
        "### Guarantee (informal but accurate)\n",
        "If calibration points are **exchangeable** with test points (i.i.d. is the common case),\n",
        "then the split conformal set satisfies:\n",
        "\n",
        "- `P( y_true \u2208 \u0393(x) ) >= 1 - \u03b1`\n",
        "\n",
        "This guarantee does **not** require the model to be correct or calibrated.\n",
        "It only relies on exchangeability and using the correct quantile construction.\n",
        "\n",
        "In finite samples, coverage is often slightly above `1-\u03b1` due to quantization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6c) (UPGRADE) Diagnostics plots\n",
        "\n",
        "We plot:\n",
        "1. **Coverage vs \u03b1** on the test set (empirical).\n",
        "2. **Histogram of set sizes** (how ambiguous predictions are).\n",
        "3. **Scatter: max probability vs set size** (how confidence relates to set cardinality).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def conformal_diagnostics(X_train_full, y_train_full, X_test, y_test, alphas):\n",
        "    # Split train into proper + calibration (fixed split for comparability across alpha)\n",
        "    X_prop, X_cal, y_prop, y_cal = train_test_split_np(X_train_full, y_train_full, test_size=0.25, seed=123, stratify=True)\n",
        "\n",
        "    # Base KNN used inside conformal wrapper\n",
        "    base_knn = KNN(\n",
        "        k=11, task=\"classification\",\n",
        "        weights=\"distance\", distance_power=2.0,\n",
        "        nominal_idx=[2, 3],\n",
        "        missing=\"impute\",\n",
        "        learn_diagonal_metric=True\n",
        "    )\n",
        "\n",
        "    coverages = []\n",
        "    mean_set_sizes = []\n",
        "\n",
        "    # Also compute per-sample confidence once for a representative alpha\n",
        "    # (we\u2019ll reuse the trained model; qhat changes with alpha)\n",
        "    base_knn.fit(X_prop, y_prop)\n",
        "    classes, proba_test = base_knn.predict_proba(X_test)\n",
        "    maxp = np.max(proba_test, axis=1)\n",
        "\n",
        "    for a in alphas:\n",
        "        conf = ConformalKNNClassifier(base_knn, alpha=float(a)).fit(X_prop, y_prop, X_cal, y_cal)\n",
        "        pred_sets = conf.predict_set(X_test)\n",
        "\n",
        "        coverage = np.mean([y_test[i] in set(pred_sets[i].tolist()) for i in range(len(y_test))])\n",
        "        sizes = np.array([len(s) for s in pred_sets], dtype=int)\n",
        "\n",
        "        coverages.append(float(coverage))\n",
        "        mean_set_sizes.append(float(np.mean(sizes)))\n",
        "\n",
        "    return np.array(coverages), np.array(mean_set_sizes), maxp\n",
        "\n",
        "alphas = np.linspace(0.01, 0.30, 16)\n",
        "coverages, mean_sizes, maxp = conformal_diagnostics(X_train, y_train, X_test, y_test, alphas)\n",
        "\n",
        "# Plot 1: Coverage vs alpha\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "plt.plot(alphas, coverages, marker=\"o\")\n",
        "plt.plot(alphas, 1.0 - alphas, linestyle=\"--\")  # target line\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"empirical coverage\")\n",
        "plt.title(\"Split conformal: coverage vs alpha\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Mean set size vs alpha\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "plt.plot(alphas, mean_sizes, marker=\"o\")\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"mean |prediction set|\")\n",
        "plt.title(\"Split conformal: mean set size vs alpha\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Set size histogram at a chosen alpha\n",
        "alpha0 = 0.10\n",
        "X_prop, X_cal, y_prop, y_cal = train_test_split_np(X_train, y_train, test_size=0.25, seed=123, stratify=True)\n",
        "knn_for_conf = KNN(\n",
        "    k=11, task=\"classification\",\n",
        "    weights=\"distance\", distance_power=2.0,\n",
        "    nominal_idx=[2, 3],\n",
        "    missing=\"impute\",\n",
        "    learn_diagonal_metric=True\n",
        ")\n",
        "conf0 = ConformalKNNClassifier(knn_for_conf, alpha=alpha0).fit(X_prop, y_prop, X_cal, y_cal)\n",
        "sets0 = conf0.predict_set(X_test)\n",
        "sizes0 = np.array([len(s) for s in sets0], dtype=int)\n",
        "\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "plt.hist(sizes0, bins=np.arange(sizes0.min(), sizes0.max() + 2) - 0.5, rwidth=0.9)\n",
        "plt.xlabel(\"|prediction set|\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.title(f\"Prediction set size histogram (alpha={alpha0})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Confidence vs set size (alpha0)\n",
        "classes, proba0 = conf0.knn.predict_proba(X_test)\n",
        "maxp0 = np.max(proba0, axis=1)\n",
        "\n",
        "plt.figure(figsize=(6.5, 4))\n",
        "plt.scatter(maxp0, sizes0, s=18)\n",
        "plt.xlabel(\"max predicted probability\")\n",
        "plt.ylabel(\"|prediction set|\")\n",
        "plt.title(f\"Confidence vs set size (alpha={alpha0})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}